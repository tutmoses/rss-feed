name: Fetch RSS Feeds

on:
  schedule:
    # Run every hour
    - cron: '0 * * * *'
  workflow_dispatch: # Allow manual trigger
  push:
    branches: [ main, master ]

jobs:
  fetch-rss:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Fetch and process RSS feeds
      run: |
        cat > fetch_rss.py << 'EOF'
        import urllib.request
        import json
        from datetime import datetime
        import re

        feeds = [
            {'url': 'https://www.radixdlt.com/post/rss.xml', 'name': 'RadixDLT'},
            {'url': 'https://ociswap.com/rss.xml', 'name': 'Ociswap'}
        ]

        items = []

        def extract_text(pattern, text):
            match = re.search(pattern, text, re.DOTALL)
            return match.group(1).strip() if match else ''

        def clean_html(text):
            # Remove HTML tags and decode entities
            text = re.sub(r'<[^>]+>', '', text)
            text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
            text = text.replace('&quot;', '"').replace('&#39;', "'").replace('&nbsp;', ' ')
            return text.strip()

        for feed in feeds:
            try:
                print(f"Fetching {feed['name']} from {feed['url']}")
                req = urllib.request.Request(feed['url'], headers={'User-Agent': 'Mozilla/5.0'})
                
                with urllib.request.urlopen(req, timeout=15) as response:
                    xml_text = response.read().decode('utf-8', errors='ignore')
                    
                    # Find all items (handles both <item> and <entry>)
                    item_pattern = r'<(?:item|entry)>(.*?)</(?:item|entry)>'
                    items_found = re.findall(item_pattern, xml_text, re.DOTALL)
                    
                    print(f"Found {len(items_found)} items in {feed['name']}")
                    
                    for item_xml in items_found:
                        # Extract title
                        title = extract_text(r'<title[^>]*>(.*?)</title>', item_xml)
                        title = clean_html(title)
                        
                        # Extract link
                        link = extract_text(r'<link[^>]*>(.*?)</link>', item_xml)
                        if not link:
                            link_match = re.search(r'<link[^>]*href=["\']([^"\']+)["\']', item_xml)
                            link = link_match.group(1) if link_match else ''
                        link = link.strip()
                        
                        # Extract description
                        desc = (extract_text(r'<description[^>]*>(.*?)</description>', item_xml) or
                               extract_text(r'<summary[^>]*>(.*?)</summary>', item_xml) or
                               extract_text(r'<content[^>]*>(.*?)</content>', item_xml))
                        desc = clean_html(desc)[:150]
                        
                        # Extract date
                        date = (extract_text(r'<pubDate[^>]*>(.*?)</pubDate>', item_xml) or
                               extract_text(r'<published[^>]*>(.*?)</published>', item_xml) or
                               extract_text(r'<updated[^>]*>(.*?)</updated>', item_xml) or
                               extract_text(r'<dc:date[^>]*>(.*?)</dc:date>', item_xml))
                        
                        if title or link:  # Only add if we have at least a title or link
                            items.append({
                                'title': title,
                                'link': link,
                                'description': desc,
                                'date': date,
                                'source': feed['name']
                            })
                            print(f"  Added: {title[:50]}...")
                        
            except Exception as e:
                print(f"Error fetching {feed['name']}: {str(e)}")
                import traceback
                traceback.print_exc()

        # Sort by date (newest first)
        items.sort(key=lambda x: x['date'], reverse=True)

        # Save to JSON
        with open('feeds.json', 'w', encoding='utf-8') as f:
            json.dump({'items': items, 'updated': datetime.utcnow().isoformat()}, f, indent=2, ensure_ascii=False)

        print(f"Successfully saved {len(items)} items to feeds.json")
        EOF

        python3 fetch_rss.py
        
    - name: Commit and push if changed
      run: |
        git config --global user.name 'RSS Bot'
        git config --global user.email 'bot@github.com'
        git add feeds.json
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update RSS feeds" && git push)
